# Arrow Electronics Embedding Project

A comprehensive demonstration of OpenShift AI for Arrow Electronics, showcasing country of origin prediction for electronic parts using BGE-Large embeddings and machine learning.

## ğŸ¯ Project Overview

This project demonstrates how to predict the country of origin for electronic parts using:
- **BGE-Large embeddings** generated via vLLM
- **K-Nearest Neighbors (KNN)** classification
- **Kubeflow Pipelines** for ML workflow orchestration
- **OpenShift AI** infrastructure

The system analyzes part descriptions and predicts whether components originate from countries like China, USA, Japan, South Korea, Germany, Taiwan, Vietnam, or Malaysia.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Synthetic     â”‚    â”‚   vLLM Server   â”‚    â”‚   Kubeflow      â”‚
â”‚   Data          â”‚â”€â”€â”€â–¶â”‚   (BGE-Large)   â”‚â”€â”€â”€â–¶â”‚   Pipeline      â”‚
â”‚   Generation    â”‚    â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                        â”‚
                                â–¼                        â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Embeddings    â”‚    â”‚   Model         â”‚
                       â”‚   Generation    â”‚    â”‚   Training &    â”‚
                       â”‚                 â”‚    â”‚   Evaluation    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
arrow-embedding/
â”œâ”€â”€ app/                          # Application code
â”‚   â”œâ”€â”€ data/                     # Dataset files
â”‚   â”‚   â”œâ”€â”€ synthetic_electronics_parts_1k.csv
â”‚   â”‚   â””â”€â”€ unseen_electronics_parts.csv
â”‚   â”œâ”€â”€ notebooks/                # Jupyter notebooks
â”‚   â”‚   â””â”€â”€ simple_demo.ipynb     # Interactive demo
â”‚   â”œâ”€â”€ scripts/                  # Utility scripts
â”‚   â”‚   â”œâ”€â”€ synthetic-data.py     # Data generation
â”‚   â”‚   â””â”€â”€ images/               # Container images
â”‚   â””â”€â”€ utils/                    # Utility modules
â”‚       â””â”€â”€ vllm_client.py        # vLLM client wrapper
â”œâ”€â”€ K8s/                          # Kubernetes/OpenShift manifests
â”‚   â”œâ”€â”€ auth/                     # Authentication setup
â”‚   â”œâ”€â”€ gpu/                      # GPU operator configuration
â”‚   â”œâ”€â”€ model-serving/            # Model serving components
â”‚   â””â”€â”€ rhoi/                     # Red Hat OpenShift AI setup
â””â”€â”€ pipeline/                     # Kubeflow pipeline
    â”œâ”€â”€ electronics_embedding_pipeline.py
    â”œâ”€â”€ electronics_embedding_pipeline.yaml
    â””â”€â”€ requirements.txt
```

## ğŸš€ Quick Start

### Prerequisites

- OpenShift cluster with Red Hat OpenShift AI (RHOAI) installed
- GPU resources available (for vLLM inference)
- MinIO or S3-compatible storage
- Python 3.9+

### 1. Setup OpenShift AI Infrastructure

```bash
# Deploy GPU operators
cd K8s/gpu
./setup.sh

# Deploy RHOAI components
cd ../rhoi
kubectl apply -f rhoai-operator-ns.yaml
kubectl apply -f rhoai-operator-group.yaml
kubectl apply -f rhoai-operator-subscription.yaml
```

### 2. Deploy vLLM Server

```bash
# Deploy BGE-Large model serving
kubectl apply -f K8s/model-serving/
```

### 3. Run the Interactive Demo

```bash
# Start Jupyter workbench
# Navigate to app/notebooks/simple_demo.ipynb
```

### 4. Execute Kubeflow Pipeline

```bash
cd pipeline
python electronics_embedding_pipeline.py
```

## ğŸ“Š Data Generation

The project includes synthetic data generation for electronics parts:

```python
from app.scripts.synthetic_data import generate_synthetic_electronics_data

# Generate 1000 synthetic samples
df = generate_synthetic_electronics_data(1000)
```

The synthetic data includes:
- **Part descriptions** with country-specific characteristics
- **8 countries of origin**: China, USA, Japan, South Korea, Germany, Taiwan, Vietnam, Malaysia
- **Realistic component types**: resistors, capacitors, microcontrollers, sensors, etc.

## ğŸ”§ Pipeline Components

### 1. Data Preparation
- Loads synthetic electronics data from MinIO/S3
- Preprocesses part descriptions
- Encodes country labels

### 2. Embedding Generation
- Connects to vLLM server running BGE-Large
- Generates 1024-dimensional embeddings for part descriptions
- Handles batch processing for efficiency

### 3. Model Training
- Trains KNN classifier (k=3) on embeddings
- Splits data into training/testing sets
- Evaluates model performance

### 4. Model Evaluation
- Generates confusion matrix
- Calculates accuracy metrics
- Creates visualization plots

### 5. Prediction on Unseen Data
- Processes new, unseen electronics parts
- Generates embeddings and predictions
- Saves results to storage

## ğŸ¯ Model Performance

The KNN classifier typically achieves:
- **Accuracy**: ~95% on test data
- **Embedding dimension**: 1024 (BGE-Large)
- **Training samples**: 800 (80% of dataset)
- **Test samples**: 200 (20% of dataset)

## ğŸ”Œ API Integration

### vLLM Client Usage

```python
from app.utils.vllm_client import create_vllm_client, get_embeddings

# Create client
client = create_vllm_client(
    endpoint="https://your-vllm-endpoint.com",
    model="bge-large",
    api_key="your-api-key"
)

# Generate embeddings
embeddings = get_embeddings(client, ["resistor 10k ohm"], "bge-large")
```

## ğŸ³ Container Images

The project includes Docker configurations for:
- **vLLM server** with BGE-Large model
- **Kubeflow pipeline components**
- **Jupyter workbench** with required dependencies

## ğŸ” Security & Authentication

- **API key authentication** for vLLM endpoints
- **OpenShift authentication** via htpasswd
- **MinIO/S3 credentials** for data storage
- **SSL certificate handling** for internal deployments

## ğŸ“ˆ Monitoring & Observability

- **GPU monitoring** via DCGM exporter
- **Model metrics** tracking in Kubeflow
- **Performance dashboards** for inference
- **Logging** across all components

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test with the provided notebooks
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the terms specified in the LICENSE file.

## ğŸ†˜ Support

For issues and questions:
1. Check the Jupyter notebook demos
2. Review the pipeline logs
3. Verify your OpenShift AI setup
4. Ensure GPU resources are available

## ğŸ”— Related Resources

- [Red Hat OpenShift AI Documentation](https://docs.redhat.com/en-us/red_hat_openshift_ai)
- [Kubeflow Pipelines Guide](https://www.kubeflow.org/docs/components/pipelines/)
- [vLLM Documentation](https://docs.vllm.ai/)
- [BGE-Large Model Card](https://huggingface.co/BAAI/bge-large-en-v1.5)
